# Use a paragraphs with a shell interpreter to execute HDFS scripts to see the files that were ingested into HDFS
hdfs dfs -ls /workshop/clickstream/data
hdfs dfs -ls /workshop/clickstream/data/products
hdfs dfs -ls /workshop/clickstream/data/users

# Use paragraphs with a %hive interpreter to execute Hive database scripts

# CREATE A DATABASE IN HIVE
CREATE DATABASE IF NOT EXISTS WORKSHOP
USE WORKSHOP

# CREATE A USERS TABLE WITH A SCHEMA ON TOP OF THE USERS.TSV FILE IN HDFS
DROP TABLE IF EXISTS USERS
CREATE EXTERNAL TABLE IF NOT EXISTS USERS 
(SWID STRING, BIRTH_DT STRING, GENDER_CD STRING) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE 
LOCATION "/workshop/clickstream/data/users"

# CONFIRM THE TABLE WAS CREATED WITH THE DATA LOADED BY EXECUTING A SELECT STATEMENT ON THE TABLE
SELECT * FROM USERS LIMIT 10

# CREATE A USERS_ORC TABLE USING THE USERS TABLE CREATED IN PREVIOUS STEP USING A CTAS STATEMENT
# THE ORC FORMAT IS AN OPTIMIZED ROW COLUMNAR FORMAT THAT SPEEDS UP THE QUERY RESPONSE
DROP TABLE IF EXISTS USERS_ORC;

CREATE TABLE IF NOT EXISTS USERS_ORC AS
SELECT * FROM USERS

# CONFIRM THE TABLE WAS CREATED WITH THE DATA LOADED BY EXECUTING A SELECT STATEMENT ON THE TABLE
SELECT * FROM USERS_ORC LIMIT 10

# CREATE A USERS TABLE WITH A SCHEMA ON TOP OF THE PRODUCTS.TSV FILE IN HDFS
DROP TABLE IF EXISTS PRODUCTS;

CREATE EXTERNAL TABLE IF NOT EXISTS PRODUCTS
(URL STRING, CATEGORY STRING, DESCRIPTION STRING) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE 
LOCATION "/workshop/clickstream/data/products"

# CONFIRM THE TABLE WAS CREATED WITH THE DATA LOADED BY EXECUTING A SELECT STATEMENT ON THE TABLE
SELECT * FROM PRODUCTS LIMIT 10

# CREATE A PRODUCTS_ORC TABLE USING THE PRODUCTS TABLE CREATED IN PREVIOUS STEP USING A CTAS STATEMENT
DROP TABLE IF EXISTS PRODUCTS_ORC;

CREATE TABLE IF NOT EXISTS PRODUCTS_ORC AS
SELECT * FROM PRODUCTS

# CONFIRM THE TABLE WAS CREATED WITH THE DATA LOADED BY EXECUTING A SELECT STATEMENT ON THE TABLE
SELECT * FROM PRODUCTS_ORC LIMIT 10


## FOR DRUID INGESTION
CREATE EXTERNAL TABLE workshop.clickstream_events (
`__time` timestamp,
clickstream_id string,
user_session_id string,
ipaddress string,
ts string,
gender string,
bday string,
is_purchased string,
is_page_errored string,
url string,
city string,
state string,
country string
)
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES (
"kafka.bootstrap.servers" = "demo.cloudera.com:6667",
"kafka.topic" = "clickstream_events",
"druid.kafka.ingestion.useEarliestOffset" = "true",
"druid.kafka.ingestion.maxRowsInMemory" = "5",
"druid.kafka.ingestion.startDelay" = "PT1S",
"druid.kafka.ingestion.period" = "PT1S",
"druid.kafka.ingestion.consumer.retries" = "2"
)

# Join with other tables
SELECT 
    CSE.TS AS TS,
    CSE.USER_SESSION_ID AS USERSESSION,
    CSE.GENDER AS GENDER,
    CSE.BDAY AS BDAY,
    CSE.IS_PURCHASED AS PURCHASED,
    CSE.IS_PAGE_ERRORED AS ERROR,
    P.CATEGORY AS PRODUCT,
    P.DESC AS DESC,
    CSE.CITY AS CITY,
    CSE.STATE AS STATE
FROM CLICKSTREAM_EVENTS CSE, PRODUCTS P 
WHERE CSE.URL = P.URL LIMIT 100

